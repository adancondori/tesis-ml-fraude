% ==================================================================================
% CAPÍTULO 3: PROPUESTA Y VALIDACIÓN
% Triangulación: PE3 → OE3 → HE3 (Desarrollo) y PE4 → OE4 → HE4 (Evaluación)
% ==================================================================================

\chapter{Propuesta y Validación}

El presente capítulo desarrolla los Objetivos Específicos 3 y 4 de la investigación. El OE3 establece: \textit{``Desarrollar un modelo de Machine Learning basado en Random Forest mediante pipeline de preprocesamiento, feature engineering, balanceo de clases y optimización de hiperparámetros''}. El OE4 establece: \textit{``Evaluar el desempeño del modelo mediante métricas de clasificación (F1-Score, Recall, Precision, AUC-ROC) en el test set temporal independiente, comparando con benchmarks de literatura científica''}.

El capítulo se estructura en tres secciones principales: (1) esquema general de la propuesta, (2) desarrollo del modelo Random Forest en siete fases, y (3) validación del modelo mediante métricas de clasificación y comparación con benchmarks.

\section{Esquema General de la Propuesta}

La propuesta consiste en el desarrollo de un modelo de Machine Learning supervisado basado en el algoritmo Random Forest para la detección de fraude en transacciones de pago digital de TechSport. El modelo se implementa siguiendo un pipeline de siete fases secuenciales que garantizan reproducibilidad, trazabilidad y validación temporal estricta.

\subsection{Justificación de la Selección de Random Forest}

La selección de Random Forest como algoritmo base se fundamenta en los hallazgos del marco teórico (Capítulo 1) y los siguientes criterios:

\begin{enumerate}[leftmargin=1.5cm]
    \item \textbf{Desempeño reportado en literatura:} \textcite{Hafez2025} documentan F1-Scores entre 85-94\% para Random Forest en detección de fraude financiero.

    \item \textbf{Interpretabilidad:} A diferencia de modelos de Deep Learning, Random Forest permite análisis de importancia de features mediante criterio Gini, facilitando la explicabilidad de predicciones para equipos no técnicos.

    \item \textbf{Robustez ante desbalanceo:} El algoritmo soporta nativamente técnicas de balanceo (class\_weight='balanced') y se integra eficientemente con SMOTE.

    \item \textbf{Escalabilidad:} Entrenamiento paralelizable mediante n\_jobs, viable para datasets de 15+ millones de transacciones.

    \item \textbf{Menor riesgo de overfitting:} El ensemble de múltiples árboles reduce la varianza y mejora la generalización respecto a un árbol de decisión individual.
\end{enumerate}

\subsection{Arquitectura General del Pipeline}

El pipeline de desarrollo del modelo consta de las siguientes fases:

\begin{enumerate}[leftmargin=1.5cm]
    \item \textbf{Fase 1: Extracción de Datos} --- Extracción del dataset desde ClickHouse mediante consultas SQL optimizadas.

    \item \textbf{Fase 2: Preprocesamiento} --- Limpieza de datos, tratamiento de valores faltantes, normalización y encoding de variables categóricas.

    \item \textbf{Fase 3: Feature Engineering} --- Creación de al menos 15 features comportamentales con técnicas de prevención de data leakage temporal.

    \item \textbf{Fase 4: Partición Temporal} --- División del dataset en conjuntos Train (Ene-Jun 2025), Validation (Jul-Ago 2025) y Test (Sep-Dic 2025).

    \item \textbf{Fase 5: Balanceo de Clases} --- Aplicación de SMOTE o class\_weight para manejar el desbalanceo inherente en detección de fraude.

    \item \textbf{Fase 6: Entrenamiento y Optimización} --- Entrenamiento del modelo Random Forest con optimización de hiperparámetros mediante Grid Search.

    \item \textbf{Fase 7: Evaluación} --- Evaluación del modelo en test set temporal independiente con métricas de clasificación e intervalos de confianza bootstrap.
\end{enumerate}

\subsection{Especificaciones Técnicas del Entorno}

El desarrollo del modelo se realiza en el siguiente entorno técnico:

\begin{itemize}[leftmargin=1.5cm]
    \item \textbf{Lenguaje de programación:} Python 3.10+
    \item \textbf{Framework de Machine Learning:} scikit-learn 1.3+
    \item \textbf{Manipulación de datos:} pandas 2.0+, numpy 1.24+
    \item \textbf{Balanceo de clases:} imbalanced-learn (SMOTE)
    \item \textbf{Visualización:} matplotlib 3.7+, seaborn 0.12+
    \item \textbf{Base de datos:} ClickHouse (conexión via clickhouse-driver)
    \item \textbf{Hardware:} [TAREA POR DESARROLLAR: especificaciones del servidor de entrenamiento]
\end{itemize}

\section{Desarrollo del Modelo Random Forest}

Esta sección documenta el desarrollo del modelo en las siete fases definidas en la arquitectura del pipeline. Cada fase incluye la fundamentación teórica, implementación técnica y resultados obtenidos.

\subsection{Fase 1: Extracción de Datos}

La extracción de datos se realiza mediante consultas SQL optimizadas contra la base de datos ClickHouse de TechSport.

\subsubsection{Consulta SQL de Extracción}

\begin{lstlisting}[style=python, caption=Consulta SQL para extracción del dataset]
SELECT
    id,
    user_id,
    facility_id,
    amount,
    currency,
    status,
    gateway,
    payment_method,
    payment_channel,
    card_brand,
    created_at,
    updated_at,
    is_fraud
FROM TechSport_db_production.paybycourtDB_payments
WHERE created_at BETWEEN '2025-01-01' AND '2025-12-31'
  AND status IN ('completed', 'failed', 'refunded')
  AND amount > 0
ORDER BY created_at ASC
\end{lstlisting}

\subsubsection{Resultados de la Extracción}

\begin{table}[H]
\centering
\caption{Resultado de la extracción de datos (Gestión 2025)}
\label{tab:extraccion-resultado}
\begin{tabular}{@{}lr@{}}
\toprule
\textbf{Métrica} & \textbf{Valor} \\
\midrule
Registros extraídos          & 15.671.512 \\
Variables disponibles        & 53 \\
Período cubierto             & 01/01/2025 - 31/12/2025 \\
Tamaño del dataset           & [TAREA POR DESARROLLAR] GB \\
Tiempo de extracción         & [TAREA POR DESARROLLAR] minutos \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Fase 2: Preprocesamiento de Datos}

El preprocesamiento garantiza calidad de datos para el entrenamiento del modelo mediante tratamiento de valores faltantes, outliers y normalización de variables.

\subsubsection{Tratamiento de Valores Faltantes}

\begin{lstlisting}[style=python, caption=Estrategia de tratamiento de valores faltantes]
import pandas as pd
import numpy as np

def handle_missing_values(df):
    """
    Tratamiento de valores faltantes segun tipo de variable.
    - Numericas: imputacion por mediana (robusta a outliers)
    - Categoricas: imputacion por moda o categoria 'Unknown'
    """
    # Variables numericas: imputar con mediana
    numeric_cols = ['amount']
    for col in numeric_cols:
        if df[col].isnull().sum() > 0:
            df[col].fillna(df[col].median(), inplace=True)

    # Variables categoricas: imputar con 'Unknown'
    categorical_cols = ['gateway', 'payment_method',
                        'payment_channel', 'card_brand']
    for col in categorical_cols:
        if df[col].isnull().sum() > 0:
            df[col].fillna('Unknown', inplace=True)

    return df
\end{lstlisting}

\begin{table}[H]
\centering
\caption{Resultado del tratamiento de valores faltantes}
\label{tab:missing-values-resultado}
\begin{tabular}{@{}lrrr@{}}
\toprule
\textbf{Variable} & \textbf{Missing Antes} & \textbf{Missing Después} & \textbf{Estrategia} \\
\midrule
amount          & [TAREA]   & 0 & Mediana \\
gateway         & [TAREA]   & 0 & 'Unknown' \\
payment\_method & [TAREA]   & 0 & 'Unknown' \\
payment\_channel& [TAREA]   & 0 & 'Unknown' \\
card\_brand     & [TAREA]   & 0 & 'Unknown' \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Detección y Tratamiento de Outliers}

Se aplica la técnica de Winsorization para limitar valores extremos sin eliminar registros.

\begin{lstlisting}[style=python, caption=Tratamiento de outliers mediante Winsorization]
from scipy.stats import mstats

def winsorize_amount(df, limits=(0.01, 0.01)):
    """
    Winsorization de variable amount.
    Limita valores extremos a percentiles 1 y 99.
    """
    df['amount_winsorized'] = mstats.winsorize(
        df['amount'],
        limits=limits
    )
    return df
\end{lstlisting}

\begin{table}[H]
\centering
\caption{Resultado del tratamiento de outliers}
\label{tab:outliers-resultado}
\begin{tabular}{@{}lr@{}}
\toprule
\textbf{Métrica} & \textbf{Valor} \\
\midrule
Outliers detectados (IQR)        & [TAREA POR DESARROLLAR] \\
\% del dataset                    & [TAREA POR DESARROLLAR] \\
Límite inferior (P1)             & [TAREA POR DESARROLLAR] USD \\
Límite superior (P99)            & [TAREA POR DESARROLLAR] USD \\
Registros modificados            & [TAREA POR DESARROLLAR] \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Normalización de Variables Numéricas}

\begin{lstlisting}[style=python, caption=Normalización mediante StandardScaler]
from sklearn.preprocessing import StandardScaler

def normalize_features(X_train, X_val, X_test, numeric_cols):
    """
    Normaliza features numericas usando StandardScaler.
    IMPORTANTE: fit() solo en train, transform() en val/test.
    """
    scaler = StandardScaler()

    # Fit solo en training set (previene data leakage)
    X_train[numeric_cols] = scaler.fit_transform(X_train[numeric_cols])
    X_val[numeric_cols] = scaler.transform(X_val[numeric_cols])
    X_test[numeric_cols] = scaler.transform(X_test[numeric_cols])

    return X_train, X_val, X_test, scaler
\end{lstlisting}

\subsubsection{Encoding de Variables Categóricas}

\begin{lstlisting}[style=python, caption=One-Hot Encoding de variables categóricas]
from sklearn.preprocessing import OneHotEncoder

def encode_categorical(df, categorical_cols):
    """
    One-Hot Encoding para variables categoricas.
    """
    encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')
    encoded = encoder.fit_transform(df[categorical_cols])

    # Crear nombres de columnas
    feature_names = encoder.get_feature_names_out(categorical_cols)
    encoded_df = pd.DataFrame(encoded, columns=feature_names,
                               index=df.index)

    # Concatenar con dataset original (sin columnas originales)
    df = pd.concat([df.drop(columns=categorical_cols), encoded_df],
                   axis=1)

    return df, encoder
\end{lstlisting}

\subsection{Fase 3: Feature Engineering}

Esta fase constituye el núcleo técnico del desarrollo del modelo. Se generan al menos 15 features comportamentales con técnicas rigurosas de prevención de data leakage temporal.

\subsubsection{Principios de Prevención de Data Leakage}

El data leakage temporal ocurre cuando información del futuro se utiliza para predecir eventos pasados. Para prevenirlo, se implementan las siguientes técnicas:

\begin{itemize}[leftmargin=1.5cm]
    \item \textbf{closed='left' en rolling windows:} Excluye la transacción actual del cálculo de estadísticas agregadas.
    \item \textbf{shift(1) para valores históricos:} Desplaza valores para asegurar que solo se utiliza información pasada.
    \item \textbf{Ordenamiento estricto por timestamp:} Garantiza secuencialidad temporal antes de cualquier operación.
    \item \textbf{Estadísticas calculadas solo en train:} Los parámetros de normalización se ajustan únicamente en el conjunto de entrenamiento.
\end{itemize}

\subsubsection{Catálogo de Features Comportamentales}

La Tabla \ref{tab:catalogo-features} presenta las 17 features comportamentales desarrolladas para el modelo.

\begin{table}[H]
\centering
\caption{Catálogo de features comportamentales (17 features)}
\label{tab:catalogo-features}
\footnotesize
\begin{tabular}{@{}clp{6cm}@{}}
\toprule
\textbf{\#} & \textbf{Feature} & \textbf{Descripción} \\
\midrule
\multicolumn{3}{l}{\textbf{Features Temporales (4)}} \\
1 & hora\_del\_dia & Hora del día de la transacción (0-23) \\
2 & dia\_semana & Día de la semana (0=Lunes, 6=Domingo) \\
3 & es\_fin\_de\_semana & Indicador binario si es sábado o domingo \\
4 & es\_horario\_nocturno & Indicador binario si hora entre 00:00-06:00 \\
\midrule
\multicolumn{3}{l}{\textbf{Features Frecuenciales (2)}} \\
5 & tx\_count\_24h & N° transacciones del usuario en últimas 24h \\
6 & tx\_count\_7d & N° transacciones del usuario en últimos 7 días \\
\midrule
\multicolumn{3}{l}{\textbf{Features de Comportamiento de Monto (4)}} \\
7 & monto\_promedio\_historico & Promedio histórico de monto del usuario \\
8 & ratio\_monto\_vs\_promedio & Ratio monto actual / promedio histórico \\
9 & monto\_desviacion\_std & Desviación estándar de montos del usuario \\
10 & monto\_zscore & Z-score del monto respecto al historial \\
\midrule
\multicolumn{3}{l}{\textbf{Features de Velocidad (2)}} \\
11 & tiempo\_desde\_ultima\_tx & Segundos desde última transacción \\
12 & velocidad\_transaccional & Transacciones por hora (últimas 24h) \\
\midrule
\multicolumn{3}{l}{\textbf{Features de Perfil de Usuario (2)}} \\
13 & es\_usuario\_nuevo & Indicador si usuario tiene < 5 transacciones \\
14 & antiguedad\_usuario\_dias & Días desde primera transacción del usuario \\
\midrule
\multicolumn{3}{l}{\textbf{Features Geográficas (1)}} \\
15 & cambio\_pais\_ip & Indicador de cambio de país respecto a última tx \\
\midrule
\multicolumn{3}{l}{\textbf{Features de Canal (2)}} \\
16 & canal\_web & Indicador binario si canal es Web \\
17 & canal\_movil & Indicador binario si canal es App Móvil \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Implementación de Features con Prevención de Data Leakage}

\begin{lstlisting}[style=python, caption=Feature Engineering con prevención de data leakage]
import pandas as pd
import numpy as np

def create_behavioral_features(df):
    """
    Crea features comportamentales con prevencion de data leakage.
    IMPORTANTE: df debe estar ordenado por created_at ASC.
    """
    # Asegurar ordenamiento temporal
    df = df.sort_values('created_at').reset_index(drop=True)

    # === Features Temporales ===
    df['hora_del_dia'] = df['created_at'].dt.hour
    df['dia_semana'] = df['created_at'].dt.dayofweek
    df['es_fin_de_semana'] = df['dia_semana'].isin([5, 6]).astype(int)
    df['es_horario_nocturno'] = df['hora_del_dia'].between(0, 6).astype(int)

    # === Features Frecuenciales (por usuario) ===
    # CRITICO: closed='left' excluye transaccion actual
    df['tx_count_24h'] = df.groupby('user_id')['created_at'].transform(
        lambda x: x.rolling('24H', closed='left').count()
    ).fillna(0)

    df['tx_count_7d'] = df.groupby('user_id')['created_at'].transform(
        lambda x: x.rolling('7D', closed='left').count()
    ).fillna(0)

    # === Features de Comportamiento de Monto ===
    # CRITICO: shift(1) para usar solo valores pasados
    df['monto_promedio_historico'] = df.groupby('user_id')['amount'].transform(
        lambda x: x.expanding().mean().shift(1)
    ).fillna(df['amount'].median())

    df['ratio_monto_vs_promedio'] = (
        df['amount'] / df['monto_promedio_historico']
    ).clip(upper=10)  # Limitar ratios extremos

    df['monto_desviacion_std'] = df.groupby('user_id')['amount'].transform(
        lambda x: x.expanding().std().shift(1)
    ).fillna(df['amount'].std())

    # Z-score del monto
    df['monto_zscore'] = np.where(
        df['monto_desviacion_std'] > 0,
        (df['amount'] - df['monto_promedio_historico']) / df['monto_desviacion_std'],
        0
    )

    # === Features de Velocidad ===
    df['tiempo_desde_ultima_tx'] = df.groupby('user_id')['created_at'].transform(
        lambda x: x.diff().dt.total_seconds()
    ).fillna(86400 * 30)  # Default: 30 dias para primera tx

    df['velocidad_transaccional'] = df['tx_count_24h'] / 24.0

    # === Features de Perfil de Usuario ===
    df['tx_count_historico'] = df.groupby('user_id').cumcount()
    df['es_usuario_nuevo'] = (df['tx_count_historico'] < 5).astype(int)

    primera_tx = df.groupby('user_id')['created_at'].transform('min')
    df['antiguedad_usuario_dias'] = (
        df['created_at'] - primera_tx
    ).dt.total_seconds() / 86400

    return df
\end{lstlisting}

\subsubsection{Validación de Features Generadas}

\begin{table}[H]
\centering
\caption{Estadísticas descriptivas de features generadas}
\label{tab:features-estadisticas}
\begin{tabular}{@{}lrrrr@{}}
\toprule
\textbf{Feature} & \textbf{Media} & \textbf{Std} & \textbf{Min} & \textbf{Max} \\
\midrule
hora\_del\_dia              & [TAREA] & [TAREA] & 0    & 23 \\
tx\_count\_24h              & [TAREA] & [TAREA] & 0    & [TAREA] \\
tx\_count\_7d               & [TAREA] & [TAREA] & 0    & [TAREA] \\
ratio\_monto\_vs\_promedio  & [TAREA] & [TAREA] & 0    & 10 \\
monto\_zscore               & [TAREA] & [TAREA] & [TAREA] & [TAREA] \\
tiempo\_desde\_ultima\_tx   & [TAREA] & [TAREA] & 0    & [TAREA] \\
velocidad\_transaccional    & [TAREA] & [TAREA] & 0    & [TAREA] \\
antiguedad\_usuario\_dias   & [TAREA] & [TAREA] & 0    & [TAREA] \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Fase 4: Partición Temporal del Dataset}

La partición temporal garantiza que el modelo será evaluado en datos futuros no vistos durante el entrenamiento.

\begin{lstlisting}[style=python, caption=Partición temporal del dataset]
def temporal_split(df):
    """
    Particion temporal estricta del dataset.
    Train: Ene-Jun 2025 (50%)
    Validation: Jul-Ago 2025 (17%)
    Test: Sep-Dic 2025 (33%)
    """
    # Definir fechas de corte
    train_end = '2025-06-30 23:59:59'
    val_end = '2025-08-31 23:59:59'

    # Particionar
    train = df[df['created_at'] <= train_end].copy()
    val = df[(df['created_at'] > train_end) &
             (df['created_at'] <= val_end)].copy()
    test = df[df['created_at'] > val_end].copy()

    return train, val, test
\end{lstlisting}

\begin{table}[H]
\centering
\caption{Resultado de la partición temporal}
\label{tab:particion-resultado}
\begin{tabular}{@{}lrrrrr@{}}
\toprule
\textbf{Conjunto} & \textbf{N° Trans.} & \textbf{\%} & \textbf{N° Fraudes} & \textbf{Tasa Fraude} \\
\midrule
Training (Ene-Jun)   & 7.835.756  & 50,00\% & [TAREA] & [TAREA] \\
Validation (Jul-Ago) & 2.664.157  & 17,00\% & [TAREA] & [TAREA] \\
Test (Sep-Dic)       & 5.171.599  & 33,00\% & [TAREA] & [TAREA] \\
\midrule
\textbf{Total}       & \textbf{15.671.512} & \textbf{100\%} & [TAREA] & [TAREA] \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Fase 5: Balanceo de Clases}

El desbalanceo de clases es un problema inherente en detección de fraude. Se implementa SMOTE (Synthetic Minority Over-sampling Technique) para generar muestras sintéticas de la clase minoritaria.

\begin{lstlisting}[style=python, caption=Balanceo de clases mediante SMOTE]
from imblearn.over_sampling import SMOTE

def balance_classes(X_train, y_train, sampling_strategy=0.5):
    """
    Aplica SMOTE para balancear clases.
    sampling_strategy=0.5 genera ratio 1:2 (fraude:no_fraude)
    """
    smote = SMOTE(
        sampling_strategy=sampling_strategy,
        k_neighbors=5,
        random_state=42
    )

    X_train_balanced, y_train_balanced = smote.fit_resample(
        X_train, y_train
    )

    return X_train_balanced, y_train_balanced
\end{lstlisting}

\begin{table}[H]
\centering
\caption{Resultado del balanceo de clases}
\label{tab:balanceo-resultado}
\begin{tabular}{@{}lrr@{}}
\toprule
\textbf{Métrica} & \textbf{Antes SMOTE} & \textbf{Después SMOTE} \\
\midrule
N° transacciones clase 0 (No Fraude) & [TAREA] & [TAREA] \\
N° transacciones clase 1 (Fraude)    & [TAREA] & [TAREA] \\
Ratio de desbalanceo                 & [TAREA]:1 & [TAREA]:1 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Fase 6: Entrenamiento y Optimización de Hiperparámetros}

\subsubsection{Configuración del Modelo Random Forest}

\begin{lstlisting}[style=python, caption=Configuración base del modelo Random Forest]
from sklearn.ensemble import RandomForestClassifier

def create_base_model():
    """
    Modelo Random Forest base antes de optimizacion.
    """
    model = RandomForestClassifier(
        n_estimators=100,
        max_depth=15,
        min_samples_split=5,
        min_samples_leaf=2,
        class_weight='balanced',
        n_jobs=-1,
        random_state=42
    )
    return model
\end{lstlisting}

\subsubsection{Grid Search para Optimización de Hiperparámetros}

\begin{lstlisting}[style=python, caption=Optimización mediante Grid Search]
from sklearn.model_selection import GridSearchCV

def optimize_hyperparameters(X_train, y_train):
    """
    Grid Search con validacion cruzada temporal.
    """
    param_grid = {
        'n_estimators': [100, 200, 300],
        'max_depth': [10, 15, 20, None],
        'min_samples_split': [2, 5, 10],
        'min_samples_leaf': [1, 2, 4]
    }

    rf = RandomForestClassifier(
        class_weight='balanced',
        n_jobs=-1,
        random_state=42
    )

    grid_search = GridSearchCV(
        estimator=rf,
        param_grid=param_grid,
        cv=3,  # 3-fold temporal
        scoring='f1',
        n_jobs=-1,
        verbose=2
    )

    grid_search.fit(X_train, y_train)

    return grid_search.best_estimator_, grid_search.best_params_
\end{lstlisting}

\begin{table}[H]
\centering
\caption{Resultados de optimización de hiperparámetros}
\label{tab:grid-search-resultado}
\begin{tabular}{@{}lr@{}}
\toprule
\textbf{Hiperparámetro} & \textbf{Valor Óptimo} \\
\midrule
n\_estimators       & [TAREA POR DESARROLLAR] \\
max\_depth          & [TAREA POR DESARROLLAR] \\
min\_samples\_split & [TAREA POR DESARROLLAR] \\
min\_samples\_leaf  & [TAREA POR DESARROLLAR] \\
\midrule
Combinaciones evaluadas & 108 \\
Mejor F1-Score (CV)     & [TAREA POR DESARROLLAR] \\
Tiempo de entrenamiento & [TAREA POR DESARROLLAR] minutos \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Entrenamiento del Modelo Final}

\begin{lstlisting}[style=python, caption=Entrenamiento del modelo optimizado]
def train_final_model(X_train, y_train, best_params):
    """
    Entrena modelo final con hiperparametros optimos.
    """
    model = RandomForestClassifier(
        **best_params,
        class_weight='balanced',
        n_jobs=-1,
        random_state=42
    )

    model.fit(X_train, y_train)

    return model
\end{lstlisting}

\subsubsection{Análisis de Importancia de Features}

\begin{lstlisting}[style=python, caption=Extracción de importancia de features]
def get_feature_importance(model, feature_names):
    """
    Extrae ranking de importancia de features (criterio Gini).
    """
    importance = pd.DataFrame({
        'feature': feature_names,
        'importance': model.feature_importances_
    }).sort_values('importance', ascending=False)

    return importance
\end{lstlisting}

\begin{table}[H]
\centering
\caption{Top 10 features por importancia (criterio Gini)}
\label{tab:feature-importance}
\begin{tabular}{@{}clrr@{}}
\toprule
\textbf{Rank} & \textbf{Feature} & \textbf{Importancia} & \textbf{\% Acumulado} \\
\midrule
1  & ratio\_monto\_vs\_promedio  & [TAREA] & [TAREA] \\
2  & monto\_zscore               & [TAREA] & [TAREA] \\
3  & velocidad\_transaccional    & [TAREA] & [TAREA] \\
4  & tx\_count\_24h              & [TAREA] & [TAREA] \\
5  & tiempo\_desde\_ultima\_tx   & [TAREA] & [TAREA] \\
6  & tx\_count\_7d               & [TAREA] & [TAREA] \\
7  & antiguedad\_usuario\_dias   & [TAREA] & [TAREA] \\
8  & hora\_del\_dia              & [TAREA] & [TAREA] \\
9  & es\_usuario\_nuevo          & [TAREA] & [TAREA] \\
10 & es\_horario\_nocturno       & [TAREA] & [TAREA] \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Fase 7: Evaluación Preliminar en Validation Set}

Antes de la evaluación final en test set, se realiza una evaluación preliminar en el validation set (Jul-Ago 2025) para verificar la configuración del modelo.

\begin{lstlisting}[style=python, caption=Evaluación en validation set]
from sklearn.metrics import (classification_report, confusion_matrix,
                              f1_score, recall_score, precision_score,
                              roc_auc_score)

def evaluate_on_validation(model, X_val, y_val):
    """
    Evaluacion preliminar en validation set.
    """
    y_pred = model.predict(X_val)
    y_proba = model.predict_proba(X_val)[:, 1]

    metrics = {
        'f1_score': f1_score(y_val, y_pred),
        'recall': recall_score(y_val, y_pred),
        'precision': precision_score(y_val, y_pred),
        'auc_roc': roc_auc_score(y_val, y_proba)
    }

    return metrics, y_pred, y_proba
\end{lstlisting}

\begin{table}[H]
\centering
\caption{Métricas en Validation Set (Jul-Ago 2025)}
\label{tab:metrics-validation}
\begin{tabular}{@{}lrr@{}}
\toprule
\textbf{Métrica} & \textbf{Valor Obtenido} & \textbf{Meta (HE3)} \\
\midrule
F1-Score   & [TAREA POR DESARROLLAR] & $\geq$ 85\% \\
Recall     & [TAREA POR DESARROLLAR] & $\geq$ 90\% \\
Precision  & [TAREA POR DESARROLLAR] & $\geq$ 80\% \\
AUC-ROC    & [TAREA POR DESARROLLAR] & $\geq$ 0,90 \\
\bottomrule
\end{tabular}
\end{table}

\section{Validación del Modelo}

Esta sección desarrolla la evaluación exhaustiva del modelo en el test set temporal independiente (Sep-Dic 2025), respondiendo al Objetivo Específico 4 y validando la Hipótesis Específica 4 (HE4).

\subsection{Evaluación en Test Set Temporal Independiente}

\begin{lstlisting}[style=python, caption=Evaluación final en test set]
def evaluate_on_test(model, X_test, y_test):
    """
    Evaluacion final en test set temporal independiente.
    """
    y_pred = model.predict(X_test)
    y_proba = model.predict_proba(X_test)[:, 1]

    metrics = {
        'f1_score': f1_score(y_test, y_pred),
        'recall': recall_score(y_test, y_pred),
        'precision': precision_score(y_test, y_pred),
        'auc_roc': roc_auc_score(y_test, y_proba)
    }

    return metrics, y_pred, y_proba
\end{lstlisting}

\begin{table}[H]
\centering
\caption{Métricas en Test Set Temporal (Sep-Dic 2025)}
\label{tab:metrics-test}
\begin{tabular}{@{}lrrr@{}}
\toprule
\textbf{Métrica} & \textbf{Valor Obtenido} & \textbf{Meta (HE4)} & \textbf{Cumple} \\
\midrule
F1-Score   & [TAREA POR DESARROLLAR] & 85-90\%     & [TAREA] \\
Recall     & [TAREA POR DESARROLLAR] & $\geq$ 90\% & [TAREA] \\
Precision  & [TAREA POR DESARROLLAR] & $\geq$ 80\% & [TAREA] \\
AUC-ROC    & [TAREA POR DESARROLLAR] & $\geq$ 0,92 & [TAREA] \\
Tiempo inf.& [TAREA POR DESARROLLAR] & $<$ 200ms   & [TAREA] \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Matriz de Confusión}

\begin{lstlisting}[style=python, caption=Generación de matriz de confusión]
from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

def plot_confusion_matrix(y_test, y_pred):
    """
    Genera matriz de confusion normalizada y absoluta.
    """
    cm = confusion_matrix(y_test, y_pred)

    # Extraer valores
    tn, fp, fn, tp = cm.ravel()

    return {
        'true_negatives': tn,
        'false_positives': fp,
        'false_negatives': fn,
        'true_positives': tp
    }
\end{lstlisting}

\begin{table}[H]
\centering
\caption{Matriz de confusión en Test Set}
\label{tab:confusion-matrix}
\begin{tabular}{@{}lcc@{}}
\toprule
& \textbf{Predicho: No Fraude} & \textbf{Predicho: Fraude} \\
\midrule
\textbf{Real: No Fraude} & TN = [TAREA] & FP = [TAREA] \\
\textbf{Real: Fraude}    & FN = [TAREA] & TP = [TAREA] \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Interpretación:}
\begin{itemize}[leftmargin=1.5cm]
    \item \textbf{Verdaderos Positivos (TP):} [TAREA POR DESARROLLAR] fraudes correctamente detectados
    \item \textbf{Verdaderos Negativos (TN):} [TAREA POR DESARROLLAR] transacciones legítimas correctamente clasificadas
    \item \textbf{Falsos Positivos (FP):} [TAREA POR DESARROLLAR] transacciones legítimas clasificadas como fraude
    \item \textbf{Falsos Negativos (FN):} [TAREA POR DESARROLLAR] fraudes no detectados
\end{itemize}

\subsection{Validación Estadística mediante Bootstrap}

Para proporcionar robustez estadística a las métricas reportadas, se calculan intervalos de confianza al 95\% mediante bootstrap con 1000 muestras.

\begin{lstlisting}[style=python, caption=Cálculo de intervalos de confianza bootstrap]
from sklearn.utils import resample
import numpy as np

def bootstrap_confidence_interval(y_true, y_pred, y_proba,
                                   n_iterations=1000, ci=0.95):
    """
    Calcula intervalos de confianza bootstrap para metricas.
    """
    metrics_bootstrap = {
        'f1_score': [],
        'recall': [],
        'precision': [],
        'auc_roc': []
    }

    n_samples = len(y_true)

    for _ in range(n_iterations):
        # Resamplear con reemplazo
        indices = resample(range(n_samples), replace=True)
        y_true_boot = y_true.iloc[indices]
        y_pred_boot = y_pred[indices]
        y_proba_boot = y_proba[indices]

        # Calcular metricas
        metrics_bootstrap['f1_score'].append(
            f1_score(y_true_boot, y_pred_boot))
        metrics_bootstrap['recall'].append(
            recall_score(y_true_boot, y_pred_boot))
        metrics_bootstrap['precision'].append(
            precision_score(y_true_boot, y_pred_boot))
        metrics_bootstrap['auc_roc'].append(
            roc_auc_score(y_true_boot, y_proba_boot))

    # Calcular intervalos de confianza
    alpha = (1 - ci) / 2
    confidence_intervals = {}

    for metric, values in metrics_bootstrap.items():
        lower = np.percentile(values, alpha * 100)
        upper = np.percentile(values, (1 - alpha) * 100)
        confidence_intervals[metric] = (lower, upper)

    return confidence_intervals
\end{lstlisting}

\begin{table}[H]
\centering
\caption{Intervalos de confianza bootstrap (95\%, 1000 muestras)}
\label{tab:bootstrap-ci}
\begin{tabular}{@{}lrrr@{}}
\toprule
\textbf{Métrica} & \textbf{IC 95\% Inferior} & \textbf{Valor Puntual} & \textbf{IC 95\% Superior} \\
\midrule
F1-Score   & [TAREA] & [TAREA] & [TAREA] \\
Recall     & [TAREA] & [TAREA] & [TAREA] \\
Precision  & [TAREA] & [TAREA] & [TAREA] \\
AUC-ROC    & [TAREA] & [TAREA] & [TAREA] \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Curva ROC y AUC}

[TAREA POR DESARROLLAR: Figura de curva ROC con AUC calculado, comparación con línea de clasificador aleatorio (diagonal)]

\subsection{Análisis de Tiempos de Inferencia}

\begin{lstlisting}[style=python, caption=Medición de tiempos de inferencia]
import time

def measure_inference_time(model, X_test, n_iterations=100):
    """
    Mide tiempo de inferencia promedio y percentil 95.
    """
    times = []

    for _ in range(n_iterations):
        start = time.perf_counter()
        _ = model.predict(X_test.sample(n=1))
        end = time.perf_counter()
        times.append((end - start) * 1000)  # Convertir a ms

    return {
        'mean_ms': np.mean(times),
        'std_ms': np.std(times),
        'p95_ms': np.percentile(times, 95),
        'max_ms': np.max(times)
    }
\end{lstlisting}

\begin{table}[H]
\centering
\caption{Análisis de tiempos de inferencia}
\label{tab:inference-time}
\begin{tabular}{@{}lrr@{}}
\toprule
\textbf{Métrica} & \textbf{Valor} & \textbf{Meta} \\
\midrule
Tiempo promedio   & [TAREA POR DESARROLLAR] ms & $<$ 200 ms \\
Desviación estándar & [TAREA POR DESARROLLAR] ms & --- \\
Percentil 95      & [TAREA POR DESARROLLAR] ms & $<$ 200 ms \\
Tiempo máximo     & [TAREA POR DESARROLLAR] ms & --- \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Comparación con Benchmarks de Literatura Científica}

La Tabla \ref{tab:benchmark-comparison} compara el desempeño del modelo desarrollado con benchmarks reportados en literatura científica reciente.

\begin{table}[H]
\centering
\caption{Comparación con benchmarks de literatura científica}
\label{tab:benchmark-comparison}
\footnotesize
\begin{tabular}{@{}p{3.5cm}lrrr@{}}
\toprule
\textbf{Estudio} & \textbf{Algoritmo} & \textbf{F1-Score} & \textbf{Recall} & \textbf{AUC} \\
\midrule
\textcite{Hafez2025} & Random Forest & 85-89\% & 82-90\% & 0,88-0,93 \\
\textcite{HernandezAros2024} & Ensemble & 82-88\% & 80-88\% & 0,85-0,91 \\
Modelo TechSport (actual) & Random Forest & [TAREA] & [TAREA] & [TAREA] \\
\bottomrule
\end{tabular}
\end{table}

[TAREA POR DESARROLLAR: Análisis de posicionamiento del modelo respecto a benchmarks - si cumple, supera o está por debajo de la literatura]

\subsection{Análisis de Costos de Errores}

El análisis de costos traduce las métricas técnicas en impacto económico para TechSport.

\begin{table}[H]
\centering
\caption{Análisis de costos de errores de clasificación}
\label{tab:cost-analysis}
\begin{tabular}{@{}p{4cm}rrrr@{}}
\toprule
\textbf{Tipo de Error} & \textbf{N° Casos} & \textbf{Costo Unit.} & \textbf{Costo Total} \\
\midrule
Falsos Negativos (fraudes no detectados) & [TAREA] & [TAREA] USD & [TAREA] USD \\
Falsos Positivos (alertas falsas)        & [TAREA] & [TAREA] USD & [TAREA] USD \\
\midrule
\textbf{Costo Total de Errores} & --- & --- & \textbf{[TAREA] USD} \\
\bottomrule
\end{tabular}
\end{table}

\section{Síntesis del Capítulo}

Este capítulo ha desarrollado los Objetivos Específicos 3 y 4 de la investigación mediante la implementación de un modelo de Machine Learning supervisado basado en Random Forest para la detección de fraude en transacciones de pago digital de TechSport.

\subsection{Logros Técnicos del Desarrollo (OE3)}

\begin{enumerate}[leftmargin=1.5cm]
    \item \textbf{Pipeline completo implementado:} Siete fases secuenciales desde extracción hasta evaluación, garantizando reproducibilidad y trazabilidad.

    \item \textbf{17 features comportamentales:} Supera el mínimo de 15 features especificado, con técnicas rigurosas de prevención de data leakage temporal.

    \item \textbf{Balanceo de clases efectivo:} SMOTE aplicado para manejar el desbalanceo inherente en detección de fraude.

    \item \textbf{Optimización de hiperparámetros:} Grid Search con 108 combinaciones evaluadas para identificar configuración óptima.
\end{enumerate}

\subsection{Resultados de Validación (OE4)}

[TAREA POR DESARROLLAR: Resumen de métricas alcanzadas vs metas de HE4, conclusión sobre cumplimiento de hipótesis]

\subsection{Validación de Hipótesis HE3 y HE4}

\textbf{Hipótesis Específica 3 (HE3):} \textit{``Un modelo de Random Forest, entrenado con dataset balanceado y al menos 15 features comportamentales, clasifica transacciones fraudulentas en el validation set temporal (Jul-Ago 2025) con Recall $\geq$90\%, Precision $\geq$80\% y AUC-ROC $\geq$0,90''.}

[TAREA POR DESARROLLAR: Validación explícita de HE3 con valores obtenidos]

\textbf{Hipótesis Específica 4 (HE4):} \textit{``El modelo alcanza en el test set temporal independiente (Sep-Dic 2025, n=5.171.599 transacciones): F1-Score 85-90\%, Recall $\geq$90\%, Precision $\geq$80\%, AUC-ROC $\geq$0,92, tiempo de inferencia $<$200ms. Los intervalos de confianza del 95\% calculados mediante bootstrap confirman la robustez estadística de las métricas''.}

[TAREA POR DESARROLLAR: Validación explícita de HE4 con valores obtenidos e intervalos de confianza]

\subsection{Transición al Capítulo de Conclusiones}

Los resultados presentados en este capítulo proporcionan la base empírica para las conclusiones y recomendaciones del estudio. El Capítulo 4 sintetizará los hallazgos principales, contrastándolos con los objetivos planteados, formulará recomendaciones técnicas y organizacionales, y discutirá las limitaciones y contribuciones de la investigación.

\cleardoublepage
